{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install biobert-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "* | Biobert |-> SBERT\n",
    "* trained on  MultiNLI, MEDNLI, ManConCorp, Our Annotation\n",
    "* maybe replacing multinli with stanford nli, or using both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/snli_1.0_train.csv\n",
      "Data/MultiNLI_cleaned.csv\n",
      "Data/manconcorpus_sent_pairs.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('Data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from biobert_embedding import downloader\n",
    "from biobert_embedding.embedding import BiobertEmbedding\n",
    "from sentence_transformers import SentenceTransformer,models\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = downloader.get_BioBert(\"google drive\")\n",
    "## downloading biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Tokens:  ['the', 'recombinant', 'protein', 'reported', 'here', ',', 'together', 'with', 'the', 'detailed', 'structural', 'information', ',', 'might', 'also', 'be', 'useful', 'to', 'others', 'developing', 'sars', '-', 'cov', '-', '2', 'diagnostics', 'and', '/', 'or', 'therapeutics', '.']\n",
      "Shape of Word Embeddings: 31 x 768\n",
      "Shape of Sentence Embedding =  768\n"
     ]
    }
   ],
   "source": [
    "text=\"the recombinant protein reported here, together with the detailed structural information, might also be useful to others developing sars-cov-2 diagnostics and/or therapeutics.\"\n",
    "\n",
    "biobert = BiobertEmbedding(model_path)\n",
    "\n",
    "word_embeddings = biobert.word_vector(text)\n",
    "sentence_embedding = biobert.sentence_vector(text)\n",
    "\n",
    "print(\"Text Tokens: \", biobert.tokens)\n",
    "#Text Tokens:  ['the', 'recombinant', 'protein', 'reported', 'here', ',', 'together', 'with', 'the', 'detailed', 'structural', 'information', ',', 'might', 'also', 'be', 'useful', 'to', 'others', 'developing', 'sars', '-', 'cov', '-', '2', 'diagnostics', 'and', '/', 'or', 'therapeutics', '.']\n",
    "\n",
    "print ('Shape of Word Embeddings: %d x %d' % (len(word_embeddings), len(word_embeddings[0])))\n",
    "# Shape of Word Embeddings: 31 x 768\n",
    "\n",
    "print(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n",
    "# Shape of Sentence Embedding =  768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERT for mapping tokens to embeddings\n",
    "from sentence_transformers import models,losses\n",
    "from sentence_transformers import SentenceTransformer\n",
    "word_embedding_model = models.BERT('./'+model_path.name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=True,\n",
    "                               pooling_mode_max_tokens=True)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model]) ## generating biobert sentence embeddings (mean pooling of sentence embedding vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sentence Embedding =  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer,SentencesDataset\n",
    "\n",
    "sentence_embeddings = model.encode([text])\n",
    "print(\"Shape of Sentence Embedding = \",len(sentence_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mancon=pd.read_csv(\"Data/manconcorpus_sent_pairs.tsv\",sep=\"\\t\").rename(columns={\"guid\":\"pairID\",\"text_a\":\"sentence1\",\n",
    "                                                                                \"text_b\":\"sentence2\"}) ## manconcorp\n",
    "\n",
    "df_snli=pd.read_csv(\"Data/snli_1.0_train.csv\") ## stanford nli\n",
    "\n",
    "df_multinli=pd.read_csv(\"Data/MultiNLI_cleaned.csv\").drop(\"Unnamed: 0\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'pairID', 'sentence1', 'sentence2'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mancon.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nli=pd.concat([df_multinli[['gold_label','sentence1','sentence2','pairID']],\n",
    "                    df_snli[['gold_label','sentence1','sentence2','pairID']]]).rename(columns={\"gold_label\":\"label\"})\n",
    "## this has snli+multinli\n",
    "df_nli=df_nli[df_nli['label']!=\"-\"]\n",
    "df_nli=df_nli.dropna(how=\"any\").reset_index(drop=True) ## removing rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataReader(object):\n",
    "    def __init__(self,dataframe):\n",
    "        self.df=dataframe.copy()\n",
    "    def get_examples(self,max_examples=0):\n",
    "        s1=self.df[\"sentence1\"].values\n",
    "        s2=self.df[\"sentence2\"].values\n",
    "        labels=self.df[\"label\"].values\n",
    "        guid=self.df[\"pairID\"].values\n",
    "        examples = []\n",
    "        for sentence_a, sentence_b, label, guid_id in zip(s1, s2, labels, guid):\n",
    "\n",
    "            examples.append(InputExample(guid=guid_id, texts=[sentence_a, sentence_b], label=self.map_label(label)))\n",
    "\n",
    "            if 0 < max_examples <= len(examples):\n",
    "                break\n",
    "\n",
    "        return examples\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_labels():\n",
    "        return {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return len(self.get_labels())\n",
    "\n",
    "    def map_label(self, label):\n",
    "        return self.get_labels()[label.strip().lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlitrain,df_nlitest=train_test_split(df_nli,test_size=0.2)\n",
    "df_nlitest,df_nlival=train_test_split(df_nlitest,test_size=0.5)\n",
    "\n",
    "df_mancontrain,df_mancontest=train_test_split(df_mancon,test_size=0.2)\n",
    "df_mancontest,df_manconval=train_test_split(df_mancontest,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 753618/753618 [05:42<00:00, 2202.19it/s]\n",
      "Convert dataset: 100%|██████████| 94203/94203 [00:43<00:00, 2164.08it/s]\n"
     ]
    }
   ],
   "source": [
    "## https://github.com/UKPLab/sentence-transformers specified here NLI training\n",
    "\n",
    "nli_reader=NLIDataReader(df_nlitrain)\n",
    "train_num_labels = nli_reader.get_num_labels()\n",
    "batch_size=32\n",
    "\n",
    "train_data = SentencesDataset(nli_reader.get_examples(), model=model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
    "\n",
    "\n",
    "val_nli_reader=NLIDataReader(df_nlival)\n",
    "\n",
    "dev_data = SentencesDataset(val_nli_reader.get_examples(), model=model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output directory (model_mnli/) already exists and is not empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-69540258d88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mevaluation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m           )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, fp16, fp16_opt_level, local_rank)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(\n\u001b[0;32m--> 306\u001b[0;31m                     output_path))\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_objectives\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Output directory (model_mnli/) already exists and is not empty."
     ]
    }
   ],
   "source": [
    "model_save_path=\"model_mnli/\"\n",
    "num_epochs=1\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 94202/94202 [00:44<00:00, 2124.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_save_path)\n",
    "\n",
    "test_nli_reader=NLIDataReader(df_nlitest)\n",
    "test_data = SentencesDataset(test_nli_reader.get_examples(), model=model)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Evaluating: 100%|██████████| 2944/2944 [03:06<00:00, 15.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27359250939614416"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing on mancon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 1791/1791 [00:01<00:00, 935.19it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mancon_reader=NLIDataReader(df_mancontest)\n",
    "test_data = SentencesDataset(test_mancon_reader.get_examples(), model=model)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Evaluating: 100%|██████████| 56/56 [00:05<00:00,  9.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.19415268509835623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training on mancon, train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 14328/14328 [00:15<00:00, 939.99it/s]\n",
      "Convert dataset: 100%|██████████| 1792/1792 [00:01<00:00, 953.15it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_save_path)\n",
    "mancon_reader=NLIDataReader(df_mancontrain)\n",
    "train_num_labels = mancon_reader.get_num_labels()\n",
    "batch_size=32\n",
    "\n",
    "train_data = SentencesDataset(mancon_reader.get_examples(), model=model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
    "\n",
    "\n",
    "val_mancon_reader=NLIDataReader(df_manconval)\n",
    "\n",
    "dev_data = SentencesDataset(val_mancon_reader.get_examples(), model=model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output directory (model_mnli_mancon/) already exists and is not empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7dcb4123cb10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mevaluation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m           )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, fp16, fp16_opt_level, local_rank)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(\n\u001b[0;32m--> 306\u001b[0;31m                     output_path))\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_objectives\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Output directory (model_mnli_mancon/) already exists and is not empty."
     ]
    }
   ],
   "source": [
    "model_save_path=\"model_mnli_mancon/\"\n",
    "num_epochs=1\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 1791/1791 [00:01<00:00, 948.93it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_save_path)\n",
    "\n",
    "test_mancon_reader=NLIDataReader(df_mancontest)\n",
    "test_data = SentencesDataset(test_mancon_reader.get_examples(), model=model)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Evaluating: 100%|██████████| 56/56 [00:05<00:00,  9.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.3187323327030114"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 05:09:14\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
